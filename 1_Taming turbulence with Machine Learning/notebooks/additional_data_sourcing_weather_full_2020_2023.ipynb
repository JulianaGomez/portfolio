{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e81c55c-f186-47cb-b464-97a088129c19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Data Sourcing\n",
    "\n",
    "- Based on Phase 3 requirements: Pull the data for recent years; Provide a clean dataset for Flights and/or Weather for Years 2020-2023.\n",
    "- Save as parquet file, with same structure as raw datasets\n",
    "\n",
    "**TO DO:**\n",
    "- Weather data is available at https://www.ncei.noaa.gov/data/local-climatological-data/archive/ up to 2024. \n",
    "- Data dictionary for weather: https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee13f961-d9a7-43f6-9731-ce0261ffac8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52673ca5-0af9-40b6-8ba1-71ce00d4ae99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blob_container = \"261storagecontainer\"  \n",
    "storage_account = \"261storage\" \n",
    "secret_scope = \"261_team_6_1_spring24_scope\"  \n",
    "secret_key = \"team_6_1_key\"  \n",
    "team_blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\" \n",
    "\n",
    "\n",
    "# blob storage is mounted here.\n",
    "mids261_mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=secret_scope, key=secret_key),\n",
    ")\n",
    "\n",
    "# see what's in the blob storage root folder\n",
    "# display(dbutils.fs.ls(f\"{team_blob_url}\"))\n",
    "\n",
    "# mount\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "# display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235c76c4-4129-480f-a7be-fa70c6907931",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72219f50-0dcf-456c-bb21-c3cc4b7821ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#standard\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F #cleaning: split, col, when, lit, concat_ws,regexp_replace, regexp_extract\n",
    "import seaborn as sns\n",
    "\n",
    "#imputing\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "#normalization and feature extraction\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler,PCA\n",
    "\n",
    "# to download files from url\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "#map\n",
    "# import folium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3f5b77-10e7-426f-bd94-59660efdc15e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Load raw data (weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a9e3da-c120-4920-84ae-65d43170e1a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather data\n",
    "df_weather = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/\")\n",
    "# display(df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee86c17e-6add-421f-a388-a9f200d21daf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Load new weather data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d3b537-4618-4dd4-a159-b7ed5de2a1ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_20_df = spark.read.parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/weather_data_2020/weather_20_parquet\")\n",
    "weather_21_df = spark.read.parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/weather_data_2021/weather_21_parquet\")\n",
    "weather_22_df = spark.read.parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/weather_data_2022/weather_22_parquet\")\n",
    "weather_23_df = spark.read.parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/weather_data_2023/weather_23_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e2e097-5439-4d18-9417-7c91db0d5796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a folder for downloaded files\n",
    "dbutils.fs.mkdirs(f\"{team_blob_url}/full_weather_data_2010_2023\") \n",
    "\n",
    "# see team blob contents\n",
    "# display(dbutils.fs.ls(f\"{team_blob_url}/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42d83f5-9b08-4109-8991-7fd331d3341a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Join all new weather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f157e563-2dcb-49a6-b701-8f5422df0ddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a variable YEAR, with the year for each dataframe (2020,2021,2022,2023)\n",
    "weather_20_df = weather_20_df.withColumn(\"YEAR\", F.lit(2020))\n",
    "weather_21_df = weather_21_df.withColumn(\"YEAR\", F.lit(2021))\n",
    "weather_22_df = weather_22_df.withColumn(\"YEAR\", F.lit(2022))\n",
    "weather_23_df = weather_23_df.withColumn(\"YEAR\", F.lit(2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8f3daa-1c0d-4cd2-b077-d9c6dd819e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather_2020_2023 = weather_20_df.union(weather_21_df).union(weather_22_df).union(weather_23_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b42127e7-b3bf-4d2c-aeb7-0c4db7eb4845",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c405e4d-57d6-471b-a34a-8d6b6d5cda1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame as a Parquet file\n",
    "df_weather_2020_2023.write.mode(\"overwrite\").parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/full_weather_data_2010_2023/df_weather_2020_2023_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64731dc-aad2-4070-acae-12cc77b033a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#load checkpointed 2023 weather\n",
    "df_weather_2020_2023 = spark.read.parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/full_weather_data_2010_2023/df_weather_2020_2023_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72607120-2557-43eb-9a79-630db4ec1cb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Sanity checks\n",
    "\n",
    "1. Numrows (df_weather_2020_2023) = numrows(weather_20_df) + numrows(weather_21_df) + numrows(weather_22_df) + numrows(weather_23_df) CHECK\n",
    "2. No additional columns: Two new variables (DYTS, DYHF) plus one added by me (source_file), but all new files have the same number of columns. CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31988cc-9d83-4aa4-aaab-080ae2e68ccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"-\"*100)\n",
    "print(\"NEW DATA\")\n",
    "print(\"-\"*100)\n",
    "print(f'2020: {weather_20_df.count()} rows and {len(weather_20_df.columns)} columns')\n",
    "print(f'2021: {weather_21_df.count()} rows and {len(weather_21_df.columns)} columns')\n",
    "print(f'2022: {weather_22_df.count()} rows and {len(weather_22_df.columns)} columns')\n",
    "print(f'2023: {weather_23_df.count()} rows and {len(weather_23_df.columns)} columns')\n",
    "print(f'2020-2023: {df_weather_2020_2023.count()} rows and {len(df_weather_2020_2023.columns)} columns')\n",
    "print(\"-\"*100)\n",
    "print(\"OLD DATA\")\n",
    "print(\"-\"*100)\n",
    "print(f'2015-2019: {df_weather.count()} rows and {len(df_weather.columns)} columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea765145-f871-4017-89de-2134570afea0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## EDA, to make sure new file is similar to previous raw_weather file\n",
    "\n",
    "1. Dimensions (see previous point)\n",
    "2. Schema - typecast all to string (expect for YEAR), to match df_weather Schema\n",
    "3. Percentage of missing columns\n",
    "4. Unique stations - they match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3660d4f5-d1f3-439e-a511-e625d979386b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather_2020_2023.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9373f1fa-c76f-4d32-a97b-7ccbe6c1bd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a939319-5b4e-4961-ac67-1a214f6d53f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove variables from new dataset which are not found in the old one: \n",
    "to_drop = ['DYTS', 'DYHF']\n",
    "df_weather_2020_2023 = df_weather_2020_2023.drop(*to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8eb159-1b43-489e-9c03-138c6052548d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Typecast to match schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c236e025-dc7a-4100-9fb5-45edcafd305c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# typecast to string\n",
    "string_cols = [c for c in df_weather_2020_2023.columns if c != 'YEAR']\n",
    "\n",
    "# Create a new DataFrame with typecasted columns\n",
    "df_new = df_weather_2020_2023.select([\n",
    "    F.when(F.col(c).isNull(), F.lit(None).cast(StringType()))\n",
    "    .otherwise(F.col(c).cast(StringType())).alias(c) if c in string_cols else F.col(c)\n",
    "    for c in df_weather_2020_2023.columns\n",
    "])\n",
    "\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679395fc-8640-4e7f-a922-9fb64d727331",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unique stations\n",
    "\n",
    "The 2020-2023 dataset has 993 fewer stations than the 2015-2019. This could be for multiple reasons: \n",
    "- a) The missing stations no longer exist/report their data (no way to check this without taking too much time)\n",
    "- b) The csv uploads into the blob went wrong. \n",
    "- c) The csv - parquet aggregation went wrong. \n",
    "\n",
    "To test options b and c, I will count the number of unique `STATION`, and make sure that it matches the number of `source_file`: When I tried manually, the decormpressed tar file output one csv per station.\n",
    "\n",
    "**CONCLUSION**\n",
    "Option a is the most likely, as the number of number of unique stations is equal to the number of unique source files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9191c3d7-f8c5-4564-b44d-acb35bbe27e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'Unique stations, 2015-2019: {df_weather.select(\"STATION\").distinct().count()}')\n",
    "print(f'Unique stations, 2020-2023: {df_weather_2020_2023.select(\"STATION\").distinct().count()}')\n",
    "\n",
    "# Find which stations from df_weather are not in df_weather_2020_2023\n",
    "missing_stations = df_weather.select(\"STATION\").subtract(df_weather_2020_2023.select(\"STATION\"))\n",
    "missing_stations_count = missing_stations.count()\n",
    "\n",
    "print(f'Stations missing in 2020-2023 data: {missing_stations_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95476bce-ee3d-43b3-a575-67380b409902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Count the number of files in each folder, along with the number of `source_file`for each year, to confirm that they are equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520bb0e6-95fa-4b77-839c-8ea2dbc1f385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#unique source files\n",
    "weather_20_count = weather_20_df.select('source_file').distinct().count()\n",
    "weather_21_count = weather_21_df.select('source_file').distinct().count()\n",
    "weather_22_count = weather_22_df.select('source_file').distinct().count()\n",
    "weather_23_count = weather_23_df.select('source_file').distinct().count()\n",
    "\n",
    "#unique stations\n",
    "weather_20_station_count = weather_20_df.select('STATION').distinct().count()\n",
    "weather_21_station_count = weather_21_df.select('STATION').distinct().count()\n",
    "weather_22_station_count = weather_22_df.select('STATION').distinct().count()\n",
    "weather_23_station_count = weather_23_df.select('STATION').distinct().count()\n",
    "\n",
    "print(f'2020 has {weather_20_count} unique source_files')\n",
    "print(f'2021 has {weather_21_count} unique source_files')\n",
    "print(f'2022 has {weather_22_count} unique source_files')\n",
    "print(f'2023 has {weather_23_count} unique source_files')\n",
    "\n",
    "# check they're the same - yes. \n",
    "assert(weather_20_count==weather_20_station_count)\n",
    "assert(weather_21_count==weather_21_station_count)\n",
    "assert(weather_22_count==weather_22_station_count)\n",
    "assert(weather_23_count==weather_23_station_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d0c74fe-4114-48fc-9bd7-a6aa6cb7ed3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Number of files per year\n",
    "\n",
    "|Year|Number of source files in data frame|Number of files when manually decompressed|Number of unique stations per year|\n",
    "|---|---|---|---|\n",
    "|2020|13562|13562|13562|\n",
    "|2021|13539|13539|13539|\n",
    "|2022|13468|13468|13468|\n",
    "|2023|13422|13422|13422|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63427fd7-affd-49df-8d22-42a3145598ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Plot all stations on a map to see if it resembles the raw 2015-2019 map\n",
    "\n",
    "(It does, just cleared output because the notebook was struggling with so much rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1cf9e4-aed8-4500-a403-c5cd144d00d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "locations_df = df_weather_2020_2023.select(\"STATION\", 'LATITUDE', 'LONGITUDE')\n",
    "locations_df = locations_df.dropDuplicates(subset=['STATION'])\n",
    "locations_df = locations_df.na.drop()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "locations_df_pd = locations_df.toPandas()\n",
    "\n",
    "# Create a map centered around the world\n",
    "world_map = folium.Map(location=[0, 0], zoom_start=2)\n",
    "\n",
    "# Define a function to add markers\n",
    "def add_marker(row):\n",
    "    station = row.STATION\n",
    "    latitude = row.LATITUDE\n",
    "    longitude = row.LONGITUDE\n",
    "    folium.Marker(\n",
    "        [float(latitude), float(longitude)],\n",
    "        popup=station,\n",
    "    ).add_to(world_map)\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "locations_df_pd.apply(add_marker, axis=1)\n",
    "\n",
    "# Display the map\n",
    "world_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c2ec85-46db-4918-aab8-c133dd45c470",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save final Parquet version to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9338ad76-e6c9-4238-b6ec-6e3edfeeb128",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_weather_2020_2023_clean = df_weather_2020_2023.drop('source_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ca8084-33a8-43c8-9401-ec55626e90b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change data format from 2023-01-03T20:00:00.000+00:00 to 2015-05-03T19:00:00\n",
    "df_weather_2020_2023_clean = df_weather_2020_2023_clean.withColumn(\n",
    "    \"DATE\",\n",
    "    F.date_format(\"DATE\", \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211b3e91-e33d-4f30-a277-452488e7c209",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Weather variables, 2015-2019\")\n",
    "print(\"-\"*30)\n",
    "df_weather.display()\n",
    "print(\"Weather variables, 2020-2023\")\n",
    "print(\"-\"*30)\n",
    "df_weather_2020_2023_clean.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e54f68d-3817-4bf4-aa5a-75a50b9fb5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert variable types in df_weather_2020_2023_clean to match the schema in df_weather\n",
    "\n",
    "for column in df_weather_2020_2023_clean.columns:\n",
    "    # Get the desired data type from df_weather\n",
    "    target_data_type = df_weather.schema[column].dataType\n",
    "    \n",
    "    # Convert the column type in df_weather_2020_2023_clean\n",
    "    df_weather_2020_2023_clean = df_weather_2020_2023_clean.withColumn(column, df_weather_2020_2023_clean[column].cast(target_data_type))\n",
    "\n",
    "\n",
    "assert df_weather_2020_2023_clean.schema == df_weather.schema, \"Schemas do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c605680-3e84-4860-a56e-b3e63894e223",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert len(df_weather_2020_2023_clean.columns) == len(df_weather.columns), \"Number of columns does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e77a56-6988-4922-82a4-c52ec9182a33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame as a Parquet file\n",
    "df_weather_2020_2023_clean.write.mode(\"overwrite\").parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/full_weather_data_2010_2023/df_weather_2020_2023_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8379584-e857-45a9-936e-7b106f68dbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#load checkpointed 2023 weather\n",
    "df_weather_2020_2023_clean = spark.read.parquet(\"wasbs://261storagecontainer@261storage.blob.core.windows.net/full_weather_data_2010_2023/df_weather_2020_2023_clean\")\n",
    "# df_weather_2020_2023_clean.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91e4cb33-20bc-447c-8979-65fb23b24b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save to DBFS\n",
    "\n",
    "- No writing privileges, so we will leave the files in our blob storage and share our credentials with Vini. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a1b7cc-baad-4ed9-8696-b7318a52e1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create new dir\n",
    "dbutils.fs.mkdirs(f\"{data_BASE_DIR}/additional_data_sourcing/team_6_1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b4eca3-4072-4383-9a74-6c2fc81a58bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save\n",
    "df_weather_2020_2023_clean.write.mode(\"overwrite\").parquet(\"dbfs:/mnt/mids-w261/additional_data_sourcing/team_6_1/df_weather_2020_2023_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42360934-d458-475b-b133-58ee4dc9efab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read\n",
    "df_weather_2020_2023_clean = spark.read.parquet(\"dbfs:/mnt/mids-w261/additional_data_sourcing/team_6_1/df_weather_2020_2023_clean\")\n",
    "df_weather_2020_2023_clean.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c8e0e02-19de-49b2-b597-8153580e5be5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Delete all csv files from blob storage (pending review by team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265a3c26-d4e4-43ee-ae40-abb22e5cb151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folders = [f\"{team_blob_url}/weather_data_2020/,{team_blob_url}/weather_data_2021/,{team_blob_url}/weather_data_2022/,{team_blob_url}/weather_data_2023/\"]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3944613696792873,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "additional_data_sourcing_weather_full_2020_2023",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
