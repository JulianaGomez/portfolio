{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5388aa2-1d8e-4551-aab6-7dd518b749f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Natural disasters, 2015-2019\n",
    "\n",
    "Sources:\n",
    "- https://www.ncei.noaa.gov/access/billions/events.pdf\n",
    "- https://www.ncei.noaa.gov/access/billions/events/US/1980-2017?disasters[]=all-disasters\n",
    "- https://www.noaa.gov/news/us-saw-10-billion-dollar-disasters-in-2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9164f8c-1119-4c60-89fb-ad59345d36be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81502ee6-3b50-4e30-b5b3-30773e925723",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blob_container = \"261storagecontainer\"  \n",
    "storage_account = \"261storage\" \n",
    "secret_scope = \"261_team_6_1_spring24_scope\"  \n",
    "secret_key = \"team_6_1_key\"  \n",
    "team_blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\" \n",
    "\n",
    "\n",
    "# blob storage is mounted here.\n",
    "mids261_mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "# SAS Token: Grant the team limited access to Azure Storage resources\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=secret_scope, key=secret_key),\n",
    ")\n",
    "\n",
    "# see what's in the blob storage root folder\n",
    "# display(dbutils.fs.ls(f\"{team_blob_url}\"))\n",
    "\n",
    "# mount\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n",
    "# display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b3d758-3e1e-4bd3-a26d-fec431902bb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a13750b-a7ff-4a73-89b8-2ce1f92c559c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#standard\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F \n",
    "import seaborn as sns\n",
    "\n",
    "# Boolean flags for sanity checks\n",
    "from pyspark.sql.types import BooleanType, ArrayType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689b3ba2-afee-4208-a0f8-7e276d2a86d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load file\n",
    "- Uploaded .csv file from Google Drive to our blob via File --> Add data\n",
    "- Queried using SQL\n",
    "- Saved as Sparkdf\n",
    "- Saved as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cec2a01-13e5-4615-b96a-316516292458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM `hive_metastore`.`default`.`natural_disasters_2015_2019_to_df_version`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0253fd23-90e5-4a3e-b733-afb3d04a8a4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "natural_disasters_2015_2019_to_df_version = _sqldf\n",
    "natural_disasters_2015_2019_to_df_version.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c78cec-17b6-4d10-8b6b-84b755fe3a0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sanity checks\n",
    "1. Check that all states are in list format\n",
    "2. Check that there are no repeated states in any list\n",
    "3. Check that there are no typos in states, based on list of all US states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fdd5cf-8551-479d-8a31-5705e70d5497",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Check that all states are in list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463a6872-db51-4324-a479-3a7052275491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Check that all states are in list format********************************************\n",
    "\n",
    "def is_list_of_strings(string):\n",
    "    import ast\n",
    "    try:\n",
    "        values = ast.literal_eval(string)\n",
    "        return isinstance(values, list) and all(isinstance(v, str) for v in values)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return False\n",
    "\n",
    "is_list_of_strings_udf = spark.udf.register(\"is_list_of_strings\", is_list_of_strings)\n",
    "\n",
    "# Trim leading/trailing whitespace and remove single quotes\n",
    "cleaned_states = natural_disasters_2015_2019_to_df_version.withColumn(\"states\", F.trim(F.regexp_replace(F.col(\"states\"), \"'\", \"\")))\n",
    "\n",
    "# Check if each string represents a list of strings\n",
    "result = cleaned_states.select(\"*\", is_list_of_strings_udf(F.col(\"states\")).alias(\"is_list_of_strings\"))\n",
    "invalid_rows = result.where(\"is_list_of_strings = false\").select(\"*\").collect()\n",
    "\n",
    "if len(invalid_rows) == 0:\n",
    "    print(\"The 'states' column contains strings representing lists of strings.\")\n",
    "else:\n",
    "    print(\"The following rows do not contain strings representing lists of strings:\")\n",
    "    for row in invalid_rows:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d603da1a-2d1e-4e58-b2c1-5b255da6c5c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_states.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bf1e61-8e00-4d3b-bec5-f4bc7dade234",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Check that there are no repeated states in any list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd34f94-cee1-4bef-8280-a376a8839755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create id column for disasters\n",
    "first_three_letters = F.lower(F.substring(F.col(\"disaster_name\"), 1, 3))\n",
    "start_date_without_hyphens = F.regexp_replace(F.col(\"start_date\"), \"-\", \"\")\n",
    "end_date_without_hyphens = F.regexp_replace(F.col(\"end_date\"), \"-\", \"\")\n",
    "disaster_id = F.concat(first_three_letters, start_date_without_hyphens,end_date_without_hyphens)\n",
    "cleaned_states = cleaned_states.withColumn(\"disaster_id\", disaster_id)\n",
    "cleaned_states.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102b59ef-b582-4dc1-a756-fa892a365ebe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check that all ids are unique. \n",
    "unique_disaster_ids = cleaned_states.select(\"disaster_id\").distinct()\n",
    "num_unique_disaster_ids = unique_disaster_ids.count()\n",
    "\n",
    "num_rows_cleaned_states = cleaned_states.count()\n",
    "\n",
    "if num_unique_disaster_ids == num_rows_cleaned_states:\n",
    "    print(\"The number of unique disaster_ids is equal to the number of rows in cleaned_states.\")\n",
    "else:\n",
    "    print(\"The number of unique disaster_ids is not equal to the number of rows in cleaned_states.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2029d6-1b6e-46ae-bfc5-1709f6a6175d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check that there are no repeated states*****************************************************\n",
    "\n",
    "# Split the states column into an array of individual states\n",
    "split_states = cleaned_states.selectExpr(\"disaster_id\", \"split(states, ',') as states_array\")\n",
    "\n",
    "# Explode the array to create one row per state\n",
    "exploded_states = split_states.selectExpr(\"disaster_id\", \"explode(states_array) as state\")\n",
    "\n",
    "# Count the occurrences of each state\n",
    "state_counts = exploded_states.groupBy(\"disaster_id\", \"state\").count()\n",
    "\n",
    "# Check if any state occurs more than once within a row\n",
    "invalid_states = state_counts.where(\"count > 1\")\n",
    "\n",
    "if invalid_states.count() == 0:\n",
    "    print(\"No repeated two-letter combinations within each row.\")\n",
    "else:\n",
    "    print(\"The following rows contain repeated two-letter combinations:\")\n",
    "    invalid_states.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc3301e-03e6-413b-b298-86cee677820b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Check that there are no typos in states, based on list of all US states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56a9981-15b9-4154-96bd-f2998290862b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#50 states, plus PR and US Virgin Islands\n",
    "valid_states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'VI', 'PR']\n",
    "\n",
    "@udf(BooleanType())\n",
    "def is_valid_state(state):\n",
    "    return state in valid_states\n",
    "\n",
    "@udf(BooleanType())\n",
    "def validate_states(states_str):\n",
    "    states = states_str.strip('[]').split(',')\n",
    "    return all(state.strip() in valid_states for state in states)\n",
    "\n",
    "# Apply the UDF to the states column\n",
    "df = cleaned_states.withColumn(\"is_valid\", validate_states(F.col(\"states\")))\n",
    "\n",
    "# Show the DataFrame with the validation result\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328bad72-c184-42ec-b21f-cbfeb079257c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "RA in disaster_id nor2017022820170301 is wrong from the base documents (both https://www.ncei.noaa.gov/access/billions/events.pdf and https://www.ncei.noaa.gov/access/billions/events/US/2015-2018?disasters[]=all-disasters), so will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841c7453-d95d-46e3-862b-3e6685d82a66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove 'RA' from the states column for disaster_id nor2017022820170301\n",
    "cleaned_states = cleaned_states.withColumn(\"states\", F.regexp_replace(\"states\", \"RA,\", \"\"))\n",
    "cleaned_states.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eccd19f0-1060-4a9f-ac83-f076b5d231a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Split states and dates into rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48990352-3eec-4153-9698-fd5361951286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove square brackets from states column in cleaned_states\n",
    "exploded_states = cleaned_states.withColumn(\"states\", F.regexp_replace(\"states\", r\"\\[\\[\\]\\]\", \"\"))\n",
    "\n",
    "# Use explode to split states in cleaned_states into individual rows, leaving one value per row\n",
    "exploded_states = exploded_states.select(\"*\", F.explode(F.split(exploded_states.states, \",\")).alias(\"state\"))\n",
    "\n",
    "# remove extra bracket\n",
    "exploded_states = exploded_states.withColumn(\"state\", F.regexp_replace(\"state\", r\"[\\[\\]]\", \"\"))\n",
    "\n",
    "# Show the exploded states DataFrame\n",
    "exploded_states.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d606ac1d-c498-4880-bbc9-27f36b0f4bc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a new dataframe with one row per date between start_date and end_date for each state\n",
    "exploded_dates = exploded_states.select(\"*\", F.explode(F.expr(\"sequence(to_date(start_date), to_date(end_date), interval 1 day)\")).alias(\"date\"))\n",
    "\n",
    "# Show the exploded dates DataFrame\n",
    "exploded_dates.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbee8493-de59-446e-9eb4-0f5baea3e581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lowercase all values in the disaster_type column\n",
    "exploded_dates = exploded_dates.withColumn(\"disaster_type\", F.lower(F.col(\"disaster_type\")))\n",
    "exploded_dates = exploded_dates.withColumn(\"disaster_type\", F.regexp_replace(F.col(\"disaster_type\"), \" \", \"_\"))\n",
    "exploded_dates.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23f9584-4f7e-4a3a-968b-8b1c19e0014c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "natural_disasters_2015_2019 = exploded_dates.select(\"disaster_name\", \"disaster_type\", \"state\", \"date\")\n",
    "natural_disasters_2015_2019.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a71858d-492c-456d-803c-fb5b749da9f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save as Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7d36a7-f17c-4555-9662-60af1e837cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save as parquet file\n",
    "natural_disasters_2015_2019.write.mode(\"overwrite\").parquet(f\"{team_blob_url}/5y_natural_disasters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83352501-2490-458b-96dc-b9751e0836af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load checkpointed file\n",
    "natural_disasters_2015_2019 = spark.read.parquet( f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net/5y_natural_disasters\" )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3544923356432521,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Natural disasters, 2015-2019",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
